In this project, we set out to build a real-time, stylized animated assistant in the form of a customizable floating 3D head that speaks and animates its face in sync with LLM-generated speech. The core challenge is transforming text responses from a large language model into speech and then driving believable facial animation—covering the mouth, jaw, eyes, eyelids, and facial deformations—that reflects both phonetic content and intended emotion. This work addresses the broader goal of making LLMs feel embodied and expressive, rather than static text boxes, by turning them into characters that can speak and react visually in real time. Our objective was to implement an end-to-end pipeline that takes a user prompt, calls an external LLM (ChatGPT) to generate a response, and then speaks that response while animating a 3D head in real time. At a minimum, we aimed to construct a neutral head mesh with morph targets for visemes and emotions, combine them within an animation controller driven by phoneme and emotion labels, and render the result using GLOO. We also sought to explore how well a finite set of discrete visemes and emotion presets can approximate natural speech and expression, and to understand the practical challenges involved in synchronizing audio, text, and animation in real time.
